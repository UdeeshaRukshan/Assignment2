# RAG System Documentation

## Overview
This document provides detailed documentation for a Retrieval-Augmented Generation (RAG) system built using LangChain and Ollama. The system enables you to query information from PDF documents using natural language questions, with responses generated by a Large Language Model (LLM) that references the content of the documents.

## Table of Contents
- [System Architecture](#system-architecture)
- [Installation Requirements](#installation-requirements)
- [Implementation Details](#implementation-details)
- [Usage Instructions](#usage-instructions)
- [Technical Components](#technical-components)
- [Customization Options](#customization-options)
- [Troubleshooting](#troubleshooting)

## System Architecture

The RAG system uses the following high-level architecture:

1. **Document Loading**: PDF documents are loaded and converted to text.
2. **Text Splitting**: Documents are split into manageable chunks.
3. **Embedding Generation**: Text chunks are converted to vector embeddings.
4. **Vector Storage**: Embeddings are stored in a Chroma vector database.
5. **Query Processing**: User queries are processed through:
   - Query embedding 
   - Similarity search for relevant chunks
   - LLM-based answer generation based on retrieved context

## Installation Requirements

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Required Packages
Install the following packages:

```bash
pip install langchain langchain-community langchain-ollama faiss-cpu chromadb sentence-transformers ollama langchain-huggingface
```

Make sure to update to the latest versions:

```bash
pip install --upgrade langchain-ollama langchain langchain-community
pip install -U langchain-huggingface
```

### Ollama Setup
1. Install Ollama by following instructions at [ollama.com](https://ollama.com)
2. Pull the required models:
   ```bash
   ollama pull llama3.2:latest
   ollama pull deepseek-r1:1.5b-qwen-distill-q4_K_M
   ```

## Implementation Details

The implementation consists of two main components:

### 1. Document Ingestion Pipeline
This component handles loading PDF documents, splitting them into chunks, creating embeddings, and storing them in a vector database.

Key parameters:
- **PDF Directory**: Current directory (`./`)
- **Vector Store**: Chroma database stored in `./chroma_db_sha`
- **Chunk Size**: 200 characters
- **Chunk Overlap**: 50 characters
- **Embedding Model**: `all-MiniLM-L6-v2` (HuggingFace)

### 2. Interactive Q&A Interface
This component provides an interactive querying system with a user-friendly interface for asking questions about the documents.

Key features:
- Interactive question answering
- Source document references
- Visual loading indicator (in notebook environment)
- Clear presentation of answers

## Usage Instructions

### Preparing Documents
1. Place PDF files in the script's directory.
2. Run the document ingestion function with `rerun=True` to force re-embedding:
   ```python
   qa = ingest_and_build(rerun=True)
   ```

### Querying Documents
After initialization, use the interactive query interface:
```python
print("Type questions below (or 'stop' to exit):")
while True:
    query = input("â–¶ ").strip()
    if query.lower() in ("stop", "exit", "quit"):
        clear_output(wait=True)
        print("Goodbye! ðŸ‘‹")
        break
    if not query:
        continue
    answer_question(qa, query)
```

Type your questions and receive answers based on the document content.

## Technical Components

### Document Loading
Uses `PyPDFLoader` from LangChain to extract text from PDF files.

```python
pdf_files = sorted(PDF_DIR.glob("*.pdf"))
documents = []
for pdf in pdf_files:
    loader = PyPDFLoader(str(pdf))
    documents.extend(loader.load())
```

### Text Splitting
Uses `RecursiveCharacterTextSplitter` to divide documents into manageable chunks.

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
)
chunks = splitter.split_documents(documents)
```

### Embeddings
Uses HuggingFace's sentence transformer model to create vector embeddings.

```python
embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)
```

### Vector Store
Uses Chroma for storing and retrieving embeddings.

```python
db = Chroma.from_documents(
    chunks,
    embedding=embeddings,
    persist_directory=str(CHROMA_DIR),
)
```

### LLM Integration
Uses Ollama to run LLMs locally.

```python
llm = Ollama(model=LLM_MODEL, temperature=0.1)
```

### Prompt Template
Uses a custom prompt to instruct the LLM:

```python
system_message = SystemMessagePromptTemplate.from_template(
    "You are a helpful AI assistant. Use ONLY the provided context to answer the user's question. "
    "If the answer is not in the context, say \"I don't know.\" Do not hallucinate."
)
```

### RetrievalQA Chain
Combines retriever with LLM to create a QA system.

```python
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": chat_prompt},
)
```

## Customization Options

### Changing Models
To use a different model, update the `LLM_MODEL` variable:

```python
LLM_MODEL = "your-preferred-model"
```

Available models depend on what you've downloaded with Ollama.

### Adjusting Retrieval Parameters
To change how many document chunks are retrieved:

```python
RETRIEVE_K = 4  # Number of relevant chunks to retrieve
```

### Modifying Chunk Size
For different document types, adjust chunking parameters:

```python
CHUNK_SIZE = 200     # Increase for more context
CHUNK_OVERLAP = 50   # Increase to reduce information loss at boundaries
```

## Troubleshooting

### Common Issues

1. **Model not found errors**:
   Ensure you've pulled the required models with Ollama:
   ```bash
   ollama pull llama3.2:latest
   ollama pull deepseek-r1:1.5b-qwen-distill-q4_K_M
   ```

2. **Empty or "I don't know" responses**:
   - Check that your PDFs were loaded correctly
   - Try adjusting the chunk size and overlap
   - Increase the number of retrieved documents (RETRIEVE_K)

3. **Slow response times**:
   - Try a smaller LLM if available
   - Reduce the number of retrieved documents
   - Check available system resources
